{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from dlf.sequential import Sequential\n",
    "from dlf.layers.dense import Dense\n",
    "from dlf.activation_functions.tanh import TanH\n",
    "from dlf.activation_functions.relu import ReLU\n",
    "from dlf.regularization.dropout import Dropout\n",
    "from dlf.optimizers.adam import AdaptiveMomentEstimation\n",
    "from dlf.normalization.batchnorm import BatchNormalization\n",
    "from dlf.trainer import Trainer\n",
    "from dlf.normalization.layernorm import LayerNormalization\n",
    "from dlf.losses.cce import CategoricalCrossEntropy\n",
    "\n",
    "from dlf.utils.dataset import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 2000, 4\n",
    "inputs = np.random.uniform(-1, 1, size=(m, n))\n",
    "y = (inputs > 0).sum(axis=1)\n",
    "\n",
    "labels = np.zeros((m, n + 1))\n",
    "labels[range(m), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs, training_labels, testing_inputs, testing_labels = train_test_split(inputs, labels, 0.7)\n",
    "testing_inputs, testing_labels, validation_inputs, validation_labels = train_test_split(testing_inputs, testing_labels, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 4) (1400, 5)\n",
      "(300, 4) (300, 5)\n",
      "(300, 4) (300, 5)\n"
     ]
    }
   ],
   "source": [
    "print(training_inputs.shape, training_labels.shape)\n",
    "print(validation_inputs.shape, validation_labels.shape)\n",
    "print(testing_inputs.shape, testing_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = (training_inputs, training_labels)\n",
    "validation_set = (validation_inputs, validation_labels)\n",
    "testing_set = (testing_inputs, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(n, 8),\n",
    "    ReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dense(8, 16),\n",
    "    ReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(keep_prob=0.3),\n",
    "    Dense(16, 32),\n",
    "    TanH(),\n",
    "    LayerNormalization(),\n",
    "    Dropout(keep_prob=0.4),\n",
    "    Dense(32, n + 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CategoricalCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdaptiveMomentEstimation(learning_rate, beta1, beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100]: Training Loss = 2.325544569453949\n",
      "[2/100]: Training Loss = 1.6660884902368764\n",
      "[3/100]: Training Loss = 1.5993710486929589\n",
      "[4/100]: Training Loss = 1.585134600522237\n",
      "[5/100]: Training Loss = 1.5451409301817414\n",
      "[6/100]: Training Loss = 1.5390186928820087\n",
      "[7/100]: Training Loss = 1.5227401714814714\n",
      "[8/100]: Training Loss = 1.5274712012175506\n",
      "[9/100]: Training Loss = 1.5157808713321241\n",
      "[10/100]: Training Loss = 1.4990774608083983\n",
      "[11/100]: Training Loss = 1.5038986420003064\n",
      "[12/100]: Training Loss = 1.4873187656530493\n",
      "[13/100]: Training Loss = 1.4809809124146998\n",
      "[14/100]: Training Loss = 1.4694016327998942\n",
      "[15/100]: Training Loss = 1.454376725549564\n",
      "[16/100]: Training Loss = 1.4323197114792385\n",
      "[17/100]: Training Loss = 1.4329014919228282\n",
      "[18/100]: Training Loss = 1.4397400321925422\n",
      "[19/100]: Training Loss = 1.4188215948037208\n",
      "[20/100]: Training Loss = 1.445575285259874\n",
      "[21/100]: Training Loss = 1.4392080042829611\n",
      "[22/100]: Training Loss = 1.4191923059115026\n",
      "[23/100]: Training Loss = 1.4232332160203107\n",
      "[24/100]: Training Loss = 1.431081756144941\n",
      "[25/100]: Training Loss = 1.420902124263926\n",
      "[26/100]: Training Loss = 1.413437957277974\n",
      "[27/100]: Training Loss = 1.4174907089009698\n",
      "[28/100]: Training Loss = 1.4170948728545258\n",
      "[29/100]: Training Loss = 1.4187723027666657\n",
      "[30/100]: Training Loss = 1.378100961710056\n",
      "[31/100]: Training Loss = 1.3947189651000804\n",
      "[32/100]: Training Loss = 1.3664851150420798\n",
      "[33/100]: Training Loss = 1.4163793505319675\n",
      "[34/100]: Training Loss = 1.4058386263874612\n",
      "[35/100]: Training Loss = 1.4075319036424885\n",
      "[36/100]: Training Loss = 1.4024021020295636\n",
      "[37/100]: Training Loss = 1.3843931026992349\n",
      "[38/100]: Training Loss = 1.4028969019195427\n",
      "[39/100]: Training Loss = 1.3889592028835334\n",
      "[40/100]: Training Loss = 1.3886792633570186\n",
      "[41/100]: Training Loss = 1.4167142855143442\n",
      "[42/100]: Training Loss = 1.382720853451821\n",
      "[43/100]: Training Loss = 1.4084867801444347\n",
      "[44/100]: Training Loss = 1.4214479017468389\n",
      "[45/100]: Training Loss = 1.3898659697484943\n",
      "[46/100]: Training Loss = 1.38406507982118\n",
      "[47/100]: Training Loss = 1.4012758088612474\n",
      "[48/100]: Training Loss = 1.407850622617356\n",
      "[49/100]: Training Loss = 1.4114687999509485\n",
      "[50/100]: Training Loss = 1.3787289361835655\n",
      "[51/100]: Training Loss = 1.376569553534098\n",
      "[52/100]: Training Loss = 1.3849848856283964\n",
      "[53/100]: Training Loss = 1.3618187972838773\n",
      "[54/100]: Training Loss = 1.3929531635962609\n",
      "[55/100]: Training Loss = 1.3885347258298244\n",
      "[56/100]: Training Loss = 1.3764697620043407\n",
      "[57/100]: Training Loss = 1.3703534677409293\n",
      "[58/100]: Training Loss = 1.3778133821007859\n",
      "[59/100]: Training Loss = 1.3716298204353377\n",
      "[60/100]: Training Loss = 1.34827744329134\n",
      "[61/100]: Training Loss = 1.3821938309494564\n",
      "[62/100]: Training Loss = 1.3537435150370425\n",
      "[63/100]: Training Loss = 1.349695694487922\n",
      "[64/100]: Training Loss = 1.3468323715863864\n",
      "[65/100]: Training Loss = 1.332617908384681\n",
      "[66/100]: Training Loss = 1.3217461782343163\n",
      "[67/100]: Training Loss = 1.3136518324802342\n",
      "[68/100]: Training Loss = 1.347979229403326\n",
      "[69/100]: Training Loss = 1.3722459676698475\n",
      "[70/100]: Training Loss = 1.330052782286016\n",
      "[71/100]: Training Loss = 1.3559974951542422\n",
      "[72/100]: Training Loss = 1.3771370114398782\n",
      "[73/100]: Training Loss = 1.3851707570255072\n",
      "[74/100]: Training Loss = 1.3654973023867094\n",
      "[75/100]: Training Loss = 1.4285873658530759\n",
      "[76/100]: Training Loss = 1.4126411238945091\n",
      "[77/100]: Training Loss = 1.352220945412678\n",
      "[78/100]: Training Loss = 1.3851121654849419\n",
      "[79/100]: Training Loss = 1.4012699406692846\n",
      "[80/100]: Training Loss = 1.3969130851877314\n",
      "[81/100]: Training Loss = 1.417987235516181\n",
      "[82/100]: Training Loss = 1.3881139319921185\n",
      "[83/100]: Training Loss = 1.4030100826952316\n",
      "[84/100]: Training Loss = 1.3798034364926992\n",
      "[85/100]: Training Loss = 1.3653892213372913\n",
      "[86/100]: Training Loss = 1.4205976788066879\n",
      "[87/100]: Training Loss = 1.3839404990121256\n",
      "[88/100]: Training Loss = 1.4566769940882718\n",
      "[89/100]: Training Loss = 1.3832590563416851\n",
      "[90/100]: Training Loss = 1.3956575905627475\n",
      "[91/100]: Training Loss = 1.403974543517499\n",
      "[92/100]: Training Loss = 1.4304125821611244\n",
      "[93/100]: Training Loss = 1.3956680269027961\n",
      "[94/100]: Training Loss = 1.3848383161418147\n",
      "[95/100]: Training Loss = 1.3583598238748023\n",
      "[96/100]: Training Loss = 1.3823627175428301\n",
      "[97/100]: Training Loss = 1.423405253036366\n",
      "[98/100]: Training Loss = 1.410837209851835\n",
      "[99/100]: Training Loss = 1.3923879092228832\n",
      "[100/100]: Training Loss = 1.4418660710974058\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(training_set, n_epochs, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ff79fb053fb88e18eab83b8116863beb7409b92540c23c781114755009620a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
