{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from dlf.sequential import Sequential\n",
    "from dlf.layers.dense import Dense\n",
    "from dlf.activation_functions.tanh import TanH\n",
    "from dlf.activation_functions.relu import ReLU\n",
    "from dlf.regularization.dropout import Dropout\n",
    "from dlf.optimizers.rmsprop import RMSProp\n",
    "from dlf.trainer import Trainer\n",
    "from dlf.activation_functions.sigmoid import Sigmoid\n",
    "from dlf.losses.bce import BinaryCrossEntropy\n",
    "\n",
    "from dlf.utils.dataset import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 2000, 4\n",
    "inputs = np.random.uniform(-1, 1, size=(m, n))\n",
    "labels = (inputs > 0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs, training_labels, testing_inputs, testing_labels = train_test_split(inputs, labels, 0.7)\n",
    "testing_inputs, testing_labels, validation_inputs, validation_labels = train_test_split(testing_inputs, testing_labels, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 4) (1400, 4)\n",
      "(300, 4) (300, 4)\n",
      "(300, 4) (300, 4)\n"
     ]
    }
   ],
   "source": [
    "print(training_inputs.shape, training_labels.shape)\n",
    "print(validation_inputs.shape, validation_labels.shape)\n",
    "print(testing_inputs.shape, testing_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = (training_inputs, training_labels)\n",
    "validation_set = (validation_inputs, validation_labels)\n",
    "testing_set = (testing_inputs, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(n, 8),\n",
    "    ReLU(),\n",
    "    Dense(8, 16),\n",
    "    ReLU(),\n",
    "    Dense(16, 32),\n",
    "    TanH(),\n",
    "    Dropout(keep_prob=0.8),\n",
    "    Dense(32, n),\n",
    "    Sigmoid()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = BinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "beta = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSProp(learning_rate, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100]: Training Loss = 0.5120123550663102\n",
      "[2/100]: Training Loss = 0.747220911212688\n",
      "[3/100]: Training Loss = 0.5170263990227131\n",
      "[4/100]: Training Loss = 0.5264579272999567\n",
      "[5/100]: Training Loss = 0.48716543576765337\n",
      "[6/100]: Training Loss = 0.4974412641889407\n",
      "[7/100]: Training Loss = 0.4162444957470213\n",
      "[8/100]: Training Loss = 0.5018406546999847\n",
      "[9/100]: Training Loss = 0.25378996355988465\n",
      "[10/100]: Training Loss = 0.16205277984450064\n",
      "[11/100]: Training Loss = 0.3278490522773251\n",
      "[12/100]: Training Loss = 0.3843136781540941\n",
      "[13/100]: Training Loss = 0.5264016434407547\n",
      "[14/100]: Training Loss = 0.4465162614636356\n",
      "[15/100]: Training Loss = 0.29449040818537836\n",
      "[16/100]: Training Loss = 0.37299209129160116\n",
      "[17/100]: Training Loss = 0.42623754329755886\n",
      "[18/100]: Training Loss = 0.2918736733935429\n",
      "[19/100]: Training Loss = 0.32659113554531116\n",
      "[20/100]: Training Loss = 0.3499849215315726\n",
      "[21/100]: Training Loss = 0.3211169183531686\n",
      "[22/100]: Training Loss = 0.4018858961162155\n",
      "[23/100]: Training Loss = 0.7394363288015294\n",
      "[24/100]: Training Loss = 0.48811972467550274\n",
      "[25/100]: Training Loss = 0.3946529896237081\n",
      "[26/100]: Training Loss = 0.4025930245123296\n",
      "[27/100]: Training Loss = 0.47862324020854097\n",
      "[28/100]: Training Loss = 0.3456465020030047\n",
      "[29/100]: Training Loss = 0.30408658656077936\n",
      "[30/100]: Training Loss = 0.3205707668400171\n",
      "[31/100]: Training Loss = 0.5078743611439505\n",
      "[32/100]: Training Loss = 0.36713553324303\n",
      "[33/100]: Training Loss = 0.28388479984137643\n",
      "[34/100]: Training Loss = 0.348358867875634\n",
      "[35/100]: Training Loss = 0.3906689384958645\n",
      "[36/100]: Training Loss = 0.545629970637265\n",
      "[37/100]: Training Loss = 0.3788848747496931\n",
      "[38/100]: Training Loss = 0.3532185199072187\n",
      "[39/100]: Training Loss = 0.4175923616184997\n",
      "[40/100]: Training Loss = 0.37250441671460083\n",
      "[41/100]: Training Loss = 0.4288887130354965\n",
      "[42/100]: Training Loss = 0.3628301480964093\n",
      "[43/100]: Training Loss = 0.33951123037346587\n",
      "[44/100]: Training Loss = 0.3756996700791997\n",
      "[45/100]: Training Loss = 0.3437868608182974\n",
      "[46/100]: Training Loss = 0.3910524797990962\n",
      "[47/100]: Training Loss = 0.43730908889318143\n",
      "[48/100]: Training Loss = 0.31593696475427585\n",
      "[49/100]: Training Loss = 0.28107713226553566\n",
      "[50/100]: Training Loss = 0.2834439369205736\n",
      "[51/100]: Training Loss = 0.27048616199646575\n",
      "[52/100]: Training Loss = 0.3256926231226162\n",
      "[53/100]: Training Loss = 0.1546429696558357\n",
      "[54/100]: Training Loss = 0.2884766582761292\n",
      "[55/100]: Training Loss = 0.3299096646221463\n",
      "[56/100]: Training Loss = 0.49422703000049656\n",
      "[57/100]: Training Loss = 0.3823558748713872\n",
      "[58/100]: Training Loss = 0.49855454881148364\n",
      "[59/100]: Training Loss = 0.4175718123827027\n",
      "[60/100]: Training Loss = 0.3198947900878601\n",
      "[61/100]: Training Loss = 0.31112483352480835\n",
      "[62/100]: Training Loss = 0.9283780529109956\n",
      "[63/100]: Training Loss = 0.2695243639432191\n",
      "[64/100]: Training Loss = 0.3772301282618941\n",
      "[65/100]: Training Loss = 0.42935824538295336\n",
      "[66/100]: Training Loss = 0.4733750478043246\n",
      "[67/100]: Training Loss = 0.7252114910220138\n",
      "[68/100]: Training Loss = 0.46898690879260624\n",
      "[69/100]: Training Loss = 0.3859805804345746\n",
      "[70/100]: Training Loss = 0.47349786072357886\n",
      "[71/100]: Training Loss = 0.4623246478830578\n",
      "[72/100]: Training Loss = 0.3372314639162624\n",
      "[73/100]: Training Loss = 0.4162406888609294\n",
      "[74/100]: Training Loss = 0.39049389615179475\n",
      "[75/100]: Training Loss = 0.331495721118633\n",
      "[76/100]: Training Loss = 0.21031328390802306\n",
      "[77/100]: Training Loss = 0.772963093333096\n",
      "[78/100]: Training Loss = 0.966521514021191\n",
      "[79/100]: Training Loss = 0.6168378149851116\n",
      "[80/100]: Training Loss = 0.32826030150620533\n",
      "[81/100]: Training Loss = 0.2868663317686035\n",
      "[82/100]: Training Loss = 0.7066245824066647\n",
      "[83/100]: Training Loss = 0.5508344787798695\n",
      "[84/100]: Training Loss = 0.35565158858611345\n",
      "[85/100]: Training Loss = 0.5253518280433715\n",
      "[86/100]: Training Loss = 0.42944710392643537\n",
      "[87/100]: Training Loss = 0.3646466459678016\n",
      "[88/100]: Training Loss = 0.4614228813858849\n",
      "[89/100]: Training Loss = 0.3244506988276052\n",
      "[90/100]: Training Loss = 0.2925887756977526\n",
      "[91/100]: Training Loss = 0.36862852270617863\n",
      "[92/100]: Training Loss = 0.38846027355846613\n",
      "[93/100]: Training Loss = 0.5792691234799507\n",
      "[94/100]: Training Loss = 0.3670969034043309\n",
      "[95/100]: Training Loss = 0.558261761483594\n",
      "[96/100]: Training Loss = 0.2720018052843716\n",
      "[97/100]: Training Loss = 0.28231694445623196\n",
      "[98/100]: Training Loss = 0.63200701812827\n",
      "[99/100]: Training Loss = 0.29129822439619346\n",
      "[100/100]: Training Loss = 0.35688807712879683\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(training_set, n_epochs, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ff79fb053fb88e18eab83b8116863beb7409b92540c23c781114755009620a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
